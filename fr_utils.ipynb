{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Function from deeplearning.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from numpy import genfromtxt\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "_FLOATX = 'float32'\n",
    "\n",
    "def variable(value, dtype=_FLOATX, name=None):\n",
    "    v = tf.Variable(np.asarray(value, dtype=dtype), name=name)\n",
    "    _get_session().run(v.initializer)\n",
    "    return v\n",
    "\n",
    "def shape(x):\n",
    "    return x.get_shape()\n",
    "\n",
    "def square(x):\n",
    "    return tf.square(x)\n",
    "\n",
    "def zeros(shape, dtype=_FLOATX, name=None):\n",
    "    return variable(np.zeros(shape), dtype, name)\n",
    "\n",
    "def concatenate(tensors, axis=-1):\n",
    "    if axis < 0:\n",
    "        axis = axis % len(tensors[0].get_shape())\n",
    "    return tf.concat(axis, tensors)\n",
    "\n",
    "def LRN2D(x):\n",
    "    return tf.nn.lrn(x, alpha=1e-4, beta=0.75)\n",
    "\n",
    "def conv2d_bn(x,\n",
    "              layer=None,\n",
    "              cv1_out=None,\n",
    "              cv1_filter=(1, 1),\n",
    "              cv1_strides=(1, 1),\n",
    "              cv2_out=None,\n",
    "              cv2_filter=(3, 3),\n",
    "              cv2_strides=(1, 1),\n",
    "              padding=None):\n",
    "    num = '' if cv2_out == None else '1'\n",
    "    tensor = Conv2D(cv1_out, cv1_filter, strides=cv1_strides, data_format='channels_first', name=layer+'_conv'+num)(x)\n",
    "    tensor = BatchNormalization(axis=1, epsilon=0.00001, name=layer+'_bn'+num)(tensor)\n",
    "    tensor = Activation('relu')(tensor)\n",
    "    if padding == None:\n",
    "        return tensor\n",
    "    tensor = ZeroPadding2D(padding=padding, data_format='channels_first')(tensor)\n",
    "    if cv2_out == None:\n",
    "        return tensor\n",
    "    tensor = Conv2D(cv2_out, cv2_filter, strides=cv2_strides, data_format='channels_first', name=layer+'_conv'+'2')(tensor)\n",
    "    tensor = BatchNormalization(axis=1, epsilon=0.00001, name=layer+'_bn'+'2')(tensor)\n",
    "    tensor = Activation('relu')(tensor)\n",
    "    return tensor\n",
    "\n",
    "WEIGHTS = [\n",
    "  'conv1', 'bn1', 'conv2', 'bn2', 'conv3', 'bn3',\n",
    "  'inception_3a_1x1_conv', 'inception_3a_1x1_bn',\n",
    "  'inception_3a_pool_conv', 'inception_3a_pool_bn',\n",
    "  'inception_3a_5x5_conv1', 'inception_3a_5x5_conv2', 'inception_3a_5x5_bn1', 'inception_3a_5x5_bn2',\n",
    "  'inception_3a_3x3_conv1', 'inception_3a_3x3_conv2', 'inception_3a_3x3_bn1', 'inception_3a_3x3_bn2',\n",
    "  'inception_3b_3x3_conv1', 'inception_3b_3x3_conv2', 'inception_3b_3x3_bn1', 'inception_3b_3x3_bn2',\n",
    "  'inception_3b_5x5_conv1', 'inception_3b_5x5_conv2', 'inception_3b_5x5_bn1', 'inception_3b_5x5_bn2',\n",
    "  'inception_3b_pool_conv', 'inception_3b_pool_bn',\n",
    "  'inception_3b_1x1_conv', 'inception_3b_1x1_bn',\n",
    "  'inception_3c_3x3_conv1', 'inception_3c_3x3_conv2', 'inception_3c_3x3_bn1', 'inception_3c_3x3_bn2',\n",
    "  'inception_3c_5x5_conv1', 'inception_3c_5x5_conv2', 'inception_3c_5x5_bn1', 'inception_3c_5x5_bn2',\n",
    "  'inception_4a_3x3_conv1', 'inception_4a_3x3_conv2', 'inception_4a_3x3_bn1', 'inception_4a_3x3_bn2',\n",
    "  'inception_4a_5x5_conv1', 'inception_4a_5x5_conv2', 'inception_4a_5x5_bn1', 'inception_4a_5x5_bn2',\n",
    "  'inception_4a_pool_conv', 'inception_4a_pool_bn',\n",
    "  'inception_4a_1x1_conv', 'inception_4a_1x1_bn',\n",
    "  'inception_4e_3x3_conv1', 'inception_4e_3x3_conv2', 'inception_4e_3x3_bn1', 'inception_4e_3x3_bn2',\n",
    "  'inception_4e_5x5_conv1', 'inception_4e_5x5_conv2', 'inception_4e_5x5_bn1', 'inception_4e_5x5_bn2',\n",
    "  'inception_5a_3x3_conv1', 'inception_5a_3x3_conv2', 'inception_5a_3x3_bn1', 'inception_5a_3x3_bn2',\n",
    "  'inception_5a_pool_conv', 'inception_5a_pool_bn',\n",
    "  'inception_5a_1x1_conv', 'inception_5a_1x1_bn',\n",
    "  'inception_5b_3x3_conv1', 'inception_5b_3x3_conv2', 'inception_5b_3x3_bn1', 'inception_5b_3x3_bn2',\n",
    "  'inception_5b_pool_conv', 'inception_5b_pool_bn',\n",
    "  'inception_5b_1x1_conv', 'inception_5b_1x1_bn',\n",
    "  'dense_layer'\n",
    "]\n",
    "\n",
    "conv_shape = {\n",
    "  'conv1': [64, 3, 7, 7],\n",
    "  'conv2': [64, 64, 1, 1],\n",
    "  'conv3': [192, 64, 3, 3],\n",
    "  'inception_3a_1x1_conv': [64, 192, 1, 1],\n",
    "  'inception_3a_pool_conv': [32, 192, 1, 1],\n",
    "  'inception_3a_5x5_conv1': [16, 192, 1, 1],\n",
    "  'inception_3a_5x5_conv2': [32, 16, 5, 5],\n",
    "  'inception_3a_3x3_conv1': [96, 192, 1, 1],\n",
    "  'inception_3a_3x3_conv2': [128, 96, 3, 3],\n",
    "  'inception_3b_3x3_conv1': [96, 256, 1, 1],\n",
    "  'inception_3b_3x3_conv2': [128, 96, 3, 3],\n",
    "  'inception_3b_5x5_conv1': [32, 256, 1, 1],\n",
    "  'inception_3b_5x5_conv2': [64, 32, 5, 5],\n",
    "  'inception_3b_pool_conv': [64, 256, 1, 1],\n",
    "  'inception_3b_1x1_conv': [64, 256, 1, 1],\n",
    "  'inception_3c_3x3_conv1': [128, 320, 1, 1],\n",
    "  'inception_3c_3x3_conv2': [256, 128, 3, 3],\n",
    "  'inception_3c_5x5_conv1': [32, 320, 1, 1],\n",
    "  'inception_3c_5x5_conv2': [64, 32, 5, 5],\n",
    "  'inception_4a_3x3_conv1': [96, 640, 1, 1],\n",
    "  'inception_4a_3x3_conv2': [192, 96, 3, 3],\n",
    "  'inception_4a_5x5_conv1': [32, 640, 1, 1,],\n",
    "  'inception_4a_5x5_conv2': [64, 32, 5, 5],\n",
    "  'inception_4a_pool_conv': [128, 640, 1, 1],\n",
    "  'inception_4a_1x1_conv': [256, 640, 1, 1],\n",
    "  'inception_4e_3x3_conv1': [160, 640, 1, 1],\n",
    "  'inception_4e_3x3_conv2': [256, 160, 3, 3],\n",
    "  'inception_4e_5x5_conv1': [64, 640, 1, 1],\n",
    "  'inception_4e_5x5_conv2': [128, 64, 5, 5],\n",
    "  'inception_5a_3x3_conv1': [96, 1024, 1, 1],\n",
    "  'inception_5a_3x3_conv2': [384, 96, 3, 3],\n",
    "  'inception_5a_pool_conv': [96, 1024, 1, 1],\n",
    "  'inception_5a_1x1_conv': [256, 1024, 1, 1],\n",
    "  'inception_5b_3x3_conv1': [96, 736, 1, 1],\n",
    "  'inception_5b_3x3_conv2': [384, 96, 3, 3],\n",
    "  'inception_5b_pool_conv': [96, 736, 1, 1],\n",
    "  'inception_5b_1x1_conv': [256, 736, 1, 1],\n",
    "}\n",
    "\n",
    "def load_weights_from_FaceNet(FRmodel):\n",
    "    # Load weights from csv files (which was exported from Openface torch model)\n",
    "    weights = WEIGHTS\n",
    "    weights_dict = load_weights()\n",
    "\n",
    "    # Set layer weights of the model\n",
    "    for name in weights:\n",
    "        if FRmodel.get_layer(name) != None:\n",
    "            FRmodel.get_layer(name).set_weights(weights_dict[name])\n",
    "        elif FRmodel.get_layer(name) != None:\n",
    "            FRmodel.get_layer(name).set_weights(weights_dict[name])\n",
    "\n",
    "def load_weights():\n",
    "    # Set weights path\n",
    "    dirPath = './weights'\n",
    "    fileNames = filter(lambda f: not f.startswith('.'), os.listdir(dirPath))\n",
    "    paths = {}\n",
    "    weights_dict = {}\n",
    "\n",
    "    for n in fileNames:\n",
    "        paths[n.replace('.csv', '')] = dirPath + '/' + n\n",
    "\n",
    "    for name in WEIGHTS:\n",
    "        if 'conv' in name:\n",
    "            conv_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n",
    "            conv_w = np.reshape(conv_w, conv_shape[name])\n",
    "            conv_w = np.transpose(conv_w, (2, 3, 1, 0))\n",
    "            conv_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n",
    "            weights_dict[name] = [conv_w, conv_b]     \n",
    "        elif 'bn' in name:\n",
    "            bn_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n",
    "            bn_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n",
    "            bn_m = genfromtxt(paths[name + '_m'], delimiter=',', dtype=None)\n",
    "            bn_v = genfromtxt(paths[name + '_v'], delimiter=',', dtype=None)\n",
    "            weights_dict[name] = [bn_w, bn_b, bn_m, bn_v]\n",
    "        elif 'dense' in name:\n",
    "            dense_w = genfromtxt(dirPath+'/dense_w.csv', delimiter=',', dtype=None)\n",
    "            dense_w = np.reshape(dense_w, (128, 736))\n",
    "            dense_w = np.transpose(dense_w, (1, 0))\n",
    "            dense_b = genfromtxt(dirPath+'/dense_b.csv', delimiter=',', dtype=None)\n",
    "            weights_dict[name] = [dense_w, dense_b]\n",
    "\n",
    "    return weights_dict\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    train_dataset = h5py.File('datasets/train_happy.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_happy.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "def img_path_to_encoding(image_path, model):\n",
    "    img1 = cv2.imread(image_path, 1)\n",
    "    return img_to_encoding(img1, model)\n",
    "    \n",
    "\n",
    "def img_to_encoding(image, model):\n",
    "    image = cv2.resize(image, (96, 96)) \n",
    "    img = image[...,::-1]\n",
    "    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
    "    x_train = np.array([img])\n",
    "    embedding = model.predict(x_train)\n",
    "    return embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
